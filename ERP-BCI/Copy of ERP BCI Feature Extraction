#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ERP-BCI Feature Extraction + CatBoost Classification

This script reproduces the steps from the provided Colab notebook:
- Loads X_train, X_test, Y_train, Y_test (.npy)
- RandomOverSampler on train (and, to match the notebook, on test as well)
- Feature extraction with mne-features (statistical, spectral, nonlinear)
- Riemannian features via XdawnCovariances + TangentSpace (pyriemann)
- Concatenates all features
- Trains CatBoostClassifier with class weights
- Prints classification reports and confusion matrices
- Saves the final feature matrices to .npy

Run:
    python erp_bci_feature_extraction.py \
        --data-dir . \
        --sfreq 128 \
        --oversample 0.30 \
        --save-features

Expected input files in --data-dir:
    X_train.npy, Y_train.npy, X_test.npy, Y_test.npy
"""

import os
import argparse
import numpy as np

from imblearn.over_sampling import RandomOverSampler

from mne_features.feature_extraction import extract_features
from pyriemann.estimation import XdawnCovariances
from pyriemann.tangentspace import TangentSpace

from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix

from catboost import CatBoostClassifier


# ---------- helpers ----------

def load_arrays(data_dir):
    def _load(name):
        path = os.path.join(data_dir, f"{name}.npy")
        if not os.path.isfile(path):
            raise FileNotFoundError(f"Required file not found: {path}")
        return np.load(path, allow_pickle=True)

    X_train = _load("X_train")
    X_test  = _load("X_test")
    Y_train = _load("Y_train")
    Y_test  = _load("Y_test")

    # ensure 1D label arrays of strings (matches notebook usage)
    Y_train = np.asarray(Y_train).reshape(-1).astype(str)
    Y_test  = np.asarray(Y_test).reshape(-1).astype(str)

    return X_train, X_test, Y_train, Y_test


def ensure_nct(X):
    """
    Ensure shape is (n_trials, n_channels, n_times).
    Assumes input is (n, a, b) and channels is the smaller of a/b (e.g., 16 vs 76).
    """
    if X.ndim != 3:
        raise ValueError(f"X must be 3D (n, ..., ...), got shape {X.shape}")
    n, a, b = X.shape
    if a <= b:
        # already (n, channels, times) if channels is smaller
        return X
    else:
        # swap to put channels first
        return np.swapaxes(X, 1, 2)


def flatten_trials(X):
    """Flatten to (n_trials, n_channels * n_times)."""
    n, c, t = X.shape
    return X.reshape(n, c * t)


def oversample_flat(arr, y, sampling_strategy):
    """Apply RandomOverSampler on a flattened (n, features) array."""
    ros = RandomOverSampler(sampling_strategy=sampling_strategy)
    X_over, y_over = ros.fit_resample(arr, y)
    return X_over, y_over


def reshape_back(flat, n_channels, n_times):
    """Reshape flattened trials back to (n_trials, n_channels, n_times)."""
    n = flat.shape[0]
    return flat.reshape(n, n_channels, n_times)


def extract_all_features(X, sfreq, fit_riemann=True, y=None, xc=None, ts=None):
    """
    Extracts the five feature blocks used in the notebook:
    - mean, variance, std
    - spect_entropy, ptp_amp
    - higuchi_fd, hurst_exp
    - wavelet_coef_energy
    - XdawnCovariances -> TangentSpace (Riemann)
    Returns (features, (XC, TS)) where features is (n, 296).
    If fit_riemann=True, fit Xdawn/TangentSpace; else transform only.
    """
    # Blocks to mirror the notebook’s order (48 + 32 + 32 + 48 + 136 = 296):
    params1 = ['mean', 'variance', 'std']                  # 3 * n_channels
    params3 = ['spect_entropy', 'ptp_amp']                 # 2 * n_channels
    params2 = ['higuchi_fd', 'hurst_exp']                  # 2 * n_channels
    params4 = ['wavelet_coef_energy']                      # 3 * n_channels (db2..db4 by default)
    # Riemann: XdawnCovariances + TangentSpace
    # Note: Requires labels for fit_transform
    if fit_riemann and y is None:
        raise ValueError("Labels y are required when fit_riemann=True.")

    # mne-features expects (n_epochs, n_channels, n_times)
    X = np.asarray(X, dtype=float)

    new_X1 = extract_features(X, sfreq, params1)  # (n, 3C)
    new_X3 = extract_features(X, sfreq, params3)  # (n, 2C)
    new_X2 = extract_features(X, sfreq, params2)  # (n, 2C)
    new_x4 = extract_features(X, sfreq, params4)  # (n, 3C)

    if fit_riemann:
        xc = XdawnCovariances(nfilter=4, estimator='oas', xdawn_estimator='oas')
        ts = TangentSpace(metric='riemann')
        x_riem = xc.fit_transform(X, y)
        x_riem = ts.fit_transform(x_riem)
    else:
        if xc is None or ts is None:
            raise ValueError("When fit_riemann=False, both xc and ts must be provided.")
        x_riem = ts.transform(xc.transform(X))

    # Concatenate in the same order as the notebook’s 'boss'
    feats = np.concatenate([new_X1, new_X3, new_X2, new_x4, x_riem], axis=1)
    return feats, (xc, ts)


def compute_class_weights(y):
    classes = np.unique(y)
    weights = compute_class_weight(class_weight='balanced', classes=classes, y=y)
    return dict(zip(classes, weights))


def train_catboost(X, y, class_weights, depth=4, iterations=35, learning_rate=0.09011926395992339):
    model = CatBoostClassifier(
        task_type='CPU',
        depth=depth,
        iterations=iterations,
        learning_rate=learning_rate,
        class_weights=class_weights,
        verbose=False
    )
    model.fit(X, y)
    return model


def print_reports(y_true, y_pred, heading):
    print(f"\n=== {heading} ===")
    print(classification_report(y_true, y_pred))
    print("Confusion matrix:")
    print(confusion_matrix(y_true, y_pred))


# ---------- main ----------

def main():
    parser = argparse.ArgumentParser(description="ERP-BCI feature extraction & CatBoost training")
    parser.add_argument("--data-dir", type=str, default=".", help="Directory containing .npy files")
    parser.add_argument("--sfreq", type=float, default=128.0, help="Sampling frequency used by mne-features")
    parser.add_argument("--oversample", type=float, default=0.30,
                        help="RandomOverSampler sampling_strategy (minority:majority ratio)")
    parser.add_argument("--save-features", action="store_true", help="Save X_train_features.npy and X_test_features.npy")
    args = parser.parse_args()

    # Load data
    X_train_raw, X_test_raw, Y_train, Y_test = load_arrays(args.data_dir)
    print(f"Loaded: X_train {X_train_raw.shape}, X_test {X_test_raw.shape}, "
          f"Y_train {Y_train.shape}, Y_test {Y_test.shape}")

    # Ensure (n, channels, times)
    X_train_nct = ensure_nct(X_train_raw)
    X_test_nct  = ensure_nct(X_test_raw)
    n, C, T = X_train_nct.shape
    print(f"Using (n, channels, times) = ({n}, {C}, {T})")

    # Flatten, oversample train (and test to match notebook)
    arr_train = flatten_trials(X_train_nct)
    arr_test  = flatten_trials(X_test_nct)

    X_over_flat, y_over = oversample_flat(arr_train, Y_train, args.oversample)
    X_test_over_flat, y_test_over = oversample_flat(arr_test, Y_test, args.oversample)

    X_over = reshape_back(X_over_flat, C, T)
    X_test_over = reshape_back(X_test_over_flat, C, T)

    print(f"Oversampled shapes -> train: {X_over.shape}, test: {X_test_over.shape}")

    # Feature extraction (fit on train, transform test)
    X_train_feats, (xc, ts) = extract_all_features(X_over, args.sfreq, fit_riemann=True, y=y_over)
    X_test_feats, _ = extract_all_features(X_test_over, args.sfreq, fit_riemann=False, xc=xc, ts=ts)

    print(f"Feature shapes -> train: {X_train_feats.shape}, test: {X_test_feats.shape}  (expected (., 296))")

    # Optionally save features
    if args.save_features:
        np.save(os.path.join(args.data_dir, "X_train_features.npy"), X_train_feats)
        np.save(os.path.join(args.data_dir, "X_test_features.npy"), X_test_feats)
        print("Saved feature matrices: X_train_features.npy, X_test_features.npy")

    # Class weights from oversampled train labels (matches notebook)
    class_weights = compute_class_weights(y_over)
    print(f"Class weights: {class_weights}")

    # Train CatBoost (using the tuned-ish params from the notebook’s later cell)
    model = train_catboost(X_train_feats, y_over, class_weights)

    # Evaluate on oversampled test (y_test_over) — as in the notebook
    y_pred_over = model.predict(X_test_feats)
    print_reports(y_test_over, y_pred_over, "Report vs OVERSAMPLED test labels")

    # Also evaluate vs original Y_test aligned to the (non-oversampled) test set size
    # NOTE: This does NOT correspond 1:1 to the oversampled test; we mimic the notebook’s second report by
    # predicting on the oversampled features but comparing with the original labels only for reference.
    try:
        # If shapes differ, we still show the oversampled-vs-original style report only if lengths match
        if len(Y_test) == len(y_pred_over):
            print_reports(Y_test, y_pred_over, "Report vs ORIGINAL test labels (same length)")
        else:
            print("\n[Info] Skipping 'vs ORIGINAL test' report because lengths differ "
                  f"(original={len(Y_test)}, oversampled_pred={len(y_pred_over)}).")
    except Exception as e:
        print(f"\n[Warning] Could not compute 'vs ORIGINAL test' report: {e}")


if __name__ == "__main__":
    main()
