#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
BCI-NER LightGBM training script (baseline + optional Optuna tuning)

Usage examples:
  python bci_lgbm_optuna.py
  python bci_lgbm_optuna.py --n-trials 100 --roc-out roc.png
  python bci_lgbm_optuna.py --x-train /path/X_train.npy --x-test /path/X_test.npy \
      --y-train /path/Y_train.npy --y-test /path/true_labels.csv
"""

import argparse
import os
import sys
import warnings

import numpy as np
import pandas as pd
from lightgbm import LGBMClassifier
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    roc_auc_score,
    roc_curve,
)
import matplotlib.pyplot as plt


def load_labels(y_path: str) -> np.ndarray:
    """
    Load labels from .npy or .csv (like the notebook's true_labels.csv).
    Returns a 1D numpy array.
    """
    if y_path.lower().endswith(".npy"):
        y = np.load(y_path)
    elif y_path.lower().endswith(".csv"):
        # notebook used header=None and reshaped to 3400; do equivalent but generic
        y = pd.read_csv(y_path, header=None).values.ravel()
    else:
        raise ValueError(f"Unsupported label format: {y_path}")
    return y.astype(int)


def evaluate(model: LGBMClassifier, X_test: np.ndarray, y_test: np.ndarray, title: str = "LightGBM"):
    """Print classification report + confusion matrix and return preds & proba."""
    preds = model.predict(X_test)
    print(classification_report(y_test, preds))
    print("------------ Confusion Matrix ---------------")
    print(confusion_matrix(y_test, preds))

    # Probabilities for ROC (class 1)
    if hasattr(model, "predict_proba"):
        proba = model.predict_proba(X_test)[:, 1]
        auc = roc_auc_score(y_test, proba)
        print(f"ROC AUC: {auc:.4f}")
        return preds, proba, auc
    return preds, None, None


def plot_roc(y_test: np.ndarray, proba: np.ndarray, title: str = "ROC of LightGBM", out_path: str | None = None):
    """Plot ROC and optionally save."""
    fpr, tpr, _ = roc_curve(y_test, proba)
    plt.figure()
    plt.plot(fpr, tpr, label=f'{title} (AUC = {roc_auc_score(y_test, proba):.2f})')
    plt.plot([0, 1], [0, 1], linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(title)
    plt.legend(loc="lower right")
    if out_path:
        plt.savefig(out_path, bbox_inches="tight", dpi=150)
        print(f"Saved ROC plot to: {out_path}")
    else:
        plt.show()
    plt.close()


def train_baseline(X_train, y_train, X_test, y_test, random_state: int = 42):
    """Train a default LGBMClassifier and evaluate."""
    print("\n=== Baseline LightGBM ===")
    lgbm = LGBMClassifier(random_state=random_state)
    lgbm.fit(X_train, y_train)
    _, proba, _ = evaluate(lgbm, X_test, y_test, title="LightGBM (baseline)")
    return lgbm, proba


def tune_with_optuna(X_train, y_train, X_test, y_test, n_trials: int = 100, random_state: int = 42):
    """Hyperparameter tuning using Optuna, then train best model and evaluate."""
    try:
        import optuna
    except ImportError:
        print("Optuna is not installed. Skipping tuning. Install with `pip install optuna`.")
        return None, None, None

    print(f"\n=== Optuna tuning (n_trials={n_trials}) ===")

    def objective(trial: "optuna.trial.Trial") -> float:
        params = {
            "objective": "binary",
            "metric": "binary_logloss",
            "boosting_type": trial.suggest_categorical("boosting_type", ["gbdt", "dart"]),
            "lambda_l1": trial.suggest_float("lambda_l1", 1e-8, 10.0, log=True),
            "lambda_l2": trial.suggest_float("lambda_l2", 1e-8, 10.0, log=True),
            "feature_fraction": trial.suggest_float("feature_fraction", 0.4, 1.0),
            "bagging_fraction": trial.suggest_float("bagging_fraction", 0.4, 1.0),
            "bagging_freq": trial.suggest_int("bagging_freq", 1, 7),
            "min_child_samples": trial.suggest_int("min_child_samples", 5, 100),
            "num_leaves": trial.suggest_int("num_leaves", 2, 64),
            "max_depth": trial.suggest_int("max_depth", -1, 12),
            "learning_rate": trial.suggest_float("learning_rate", 1e-3, 0.2, log=True),
            # keep other defaults consistent with notebook intent
            "random_state": random_state,
            "n_estimators": trial.suggest_int("n_estimators", 50, 500),
            "n_jobs": -1,
        }
        model = LGBMClassifier(**params)
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        return accuracy_score(y_test, preds)

    sampler = None  # default TPE
    study = optuna.create_study(direction="maximize", sampler=sampler)
    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)

    print(f"Number of finished trials: {len(study.trials)}")
    print(f"Best params: {study.best_trial.params}")
    print(f"Best accuracy: {study.best_value:.6f}")

    best_params = study.best_trial.params
    tuned = LGBMClassifier(**best_params)
    tuned.fit(X_train, y_train)
    _, proba, auc = evaluate(tuned, X_test, y_test, title="LightGBM (tuned)")
    return tuned, proba, auc


def parse_args():
    parser = argparse.ArgumentParser(description="Train LightGBM on BCI-NER features with optional Optuna tuning.")
    # defaults match the notebook paths; override as needed
    base = "/content/drive/MyDrive/2021_VIIT08_P300/Dataset/bci-ner"
    parser.add_argument("--x-train", default=os.path.join(base, "Proposed Feature Extraction using mne", "X_train.npy"))
    parser.add_argument("--x-test",  default=os.path.join(base, "Proposed Feature Extraction using mne", "X_test.npy"))
    parser.add_argument("--y-train", default=os.path.join(base, "Y_train.npy"))
    parser.add_argument("--y-test",  default=os.path.join(base, "true_labels.csv"))
    parser.add_argument("--n-trials", type=int, default=100, help="Optuna trials; set 0 to skip tuning.")
    parser.add_argument("--roc-out", default=None, help="Path to save ROC PNG (if provided).")
    parser.add_argument("--no-plot", action="store_true", help="Do not show ROC plots.")
    parser.add_argument("--seed", type=int, default=42)
    return parser.parse_args()


def main():
    warnings.filterwarnings("ignore", category=UserWarning)
    args = parse_args()

    # Load data
    print("Loading data...")
    X_train = np.load(args.x_train)
    X_test = np.load(args.x_test)
    y_train = load_labels(args.y_train)
    y_test = load_labels(args.y_test)

    # Sanity shapes
    print(f"X_train: {X_train.shape}  y_train: {y_train.shape}")
    print(f"X_test:  {X_test.shape}   y_test:  {y_test.shape}")

    # Baseline
    baseline_model, base_proba = train_baseline(X_train, y_train, X_test, y_test, random_state=args.seed)
    if (base_proba is not None) and (not args.no_plot):
        plot_roc(y_test, base_proba, title="ROC of LightGBM (baseline)", out_path=args.roc_out)

    # Tuning (optional)
    tuned_model = None
    tuned_proba = None
    if args.n_trials and args.n_trials > 0:
        tuned_model, tuned_proba, _ = tune_with_optuna(
            X_train, y_train, X_test, y_test, n_trials=args.n_trials, random_state=args.seed
        )
        if (tuned_proba is not None) and (not args.no_plot):
            # If a single output path was provided, suffix it for tuned run to avoid overwrite
            out = None
            if args.roc_out:
                root, ext = os.path.splitext(args.roc_out)
                out = f"{root}_tuned{ext}"
            plot_roc(y_test, tuned_proba, title="ROC of LightGBM (tuned)", out_path=out)


if __name__ == "__main__":
    main()
